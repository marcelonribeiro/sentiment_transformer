# Transformer-based Sentiment Analysis

This project implements a sentiment classification model (positive or negative) for Brazilian e-commerce product reviews, utilizing a Transformer-based architecture. The model is trained on the [B2W-Reviews01](https://www.kaggle.com/datasets/involvest-data-lab/b2w-reviews01) dataset and leverages pre-trained fastText word embeddings for the Portuguese language.

## Key Features

-   **Model**: Utilizes a Transformer Encoder architecture to capture the context of words within sentences, leading to more accurate sentiment classification.
-   **Embeddings**: Employs pre-trained word vectors from [fastText for Portuguese](https://fasttext.cc/docs/en/crawl-vectors.html) (`cc.pt.300.vec`), providing the model with a robust initial semantic understanding.
-   **Complete Pipeline**: The project is divided into two main scripts:
    1.  `train_model.py`: Handles data preprocessing, model building, training, and saving the resulting artifacts (model, vectorizer, and test data).
    2.  `evaluate_model.py`: Loads the saved artifacts, evaluates the model's performance on the test set, and generates visualizations like the confusion matrix and ROC curve.
-   **Data Balancing**: Applies a sampling strategy to balance the positive and negative sentiment classes, preventing the model from becoming biased.
-   **Frameworks**: Built with TensorFlow and Keras, leveraging the `tf.data` API for an efficient training pipeline.

## File Structure

```
.
├── data/
│   ├── B2W-Reviews01.csv       # Original dataset
│   └── cc.pt.300.vec           # fastText embeddings (download required)
├── models/
│   ├── sentiment_transformer.keras # Trained model (generated by train_model.py)
│   ├── test_data.pkl           # Test data (generated by script train_model.py)
│   └── vectorizer.pkl          # Vectorization layer (generated by script train_model.py)
├── train_model.py           # Script for model training
├── evaluate_model.py        # Script for model evaluation
├── transformer_layers.py       # Custom Transformer layer definitions
└── requirements.txt            # Project dependencies
```

## Prerequisites

-   Python 3.8+
-   TensorFlow 2.x
-   Pandas
-   Scikit-learn
-   Gensim
-   Matplotlib

## Installation

1.  **Clone the repository:**
    ```sh
    git clone [https://github.com/marcelonribeiro/sentiment_transformer.git](https://github.com/marcelonribeiro/sentiment_transformer.git)
    cd your-repository
    ```

2.  **Create and activate a virtual environment (recommended):**
    ```sh
    python -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```

3.  **Install the dependencies:**
    ```sh
    pip install -r requirements.txt
    ```
    *If the `requirements.txt` file does not exist, you can create it with the following content:*
    ```txt
    tensorflow
    pandas
    scikit-learn
    gensim
    matplotlib
    numpy
    ```

## How to Use

### 1. Data Preparation

Before running the scripts, you need to download the required data:

1.  **fastText Embeddings**: Download the word vectors for Portuguese ("pt") from the [official fastText website](https://fasttext.cc/docs/en/crawl-vectors.html) (the `cc.pt.300.vec.gz` file). Unzip it and place the `cc.pt.300.vec` file in the `data/` folder.

### 2. Model Training

Run the training script. It will process the data, train the Transformer model, and save the artifacts in the `models/` folder.

```sh
python train_model.py
```

The script will display the training progress in the console. At the end, you will see messages confirming that the model, vectorizer, and test data have been saved.

### 3. Model Evaluation

After training is complete, run the evaluation script to measure the model's performance on the test dataset.

```sh
python evaluate_model.py
```

This script will print the following metrics to the console:
-   Accuracy
-   Precision
-   Recall
-   F1-Score
-   AUC

Additionally, it will display two plots: the **Confusion Matrix** and the **ROC Curve**.

## Evaluation Results

After running the `evaluate_model.py` script, you will obtain performance metrics and the following visualizations:

#### Confusion Matrix
Shows the relationship between the true labels and the model's predictions, helping to visualize the hits and misses for each class (Negative vs. Positive).

#### ROC Curve
Illustrates the model's ability to distinguish between classes. An AUC (Area Under the Curve) value close to 1.0 indicates excellent performance.

## About the Transformer Layers

The `transformer_layers.py` file contains the custom implementations of the layers that make up the Transformer's Encoder block:

-   `PositionalEncoding`: Adds positional information to the input embeddings, allowing the model to understand the order of words.
-   `MultiHeadAttention`: Allows the model to focus on different parts of the input sequence simultaneously.
-   `EncoderBlock`: The fundamental building block of the Encoder, composed of a Multi-Head Attention layer and a Feed-Forward network, with residual connections and layer normalization.